

(16 June – 28 September 2025)


|Week|Date Range|ML Weekly Goals|DL Weekly Goals|
|---|---|---|---|
|**1**|16 – 22 Jun|- Refresh descriptive statistics, probability basics  <br>- Perform full EDA on Titanic data with pandas & visualizations|- Review perceptron & back-prop math  <br>- Code NumPy 2-layer MLP on MNIST (≥92 % acc)|
|**2**|23 – 29 Jun|- Understand regularisation (L2, early-stopping)  <br>- Add both to last week’s logistic-reg notebook|- Install & explore PyTorch (“60-min Blitz”)  <br>- Re-implement Week 1 MLP in PyTorch/GPU|
|**3**|30 Jun – 6 Jul|- Learn Decision Trees, Random Forests, Gradient Boosting  <br>- Compare models on Titanic; analyse feature importances|- Study CNN building blocks (conv, pool)  <br>- Train simple CNN on CIFAR-10 (≥60 % acc)|
|**4**|7 – 13 Jul|- Dive into Support-Vector Machines & kernels  <br>- Benchmark SVM vs RF on digits dataset|- Practise transfer learning: fine-tune ResNet-18 on a 200-image custom set|
|**5**|14 – 20 Jul|- Cover unsupervised learning: K-means, PCA  <br>- Cluster Mall-Customers dataset; 2-D PCA plot|- Introduce RNNs/LSTMs conceptually  <br>- Build character-level LSTM text generator|
|**6**|21 – 27 Jul|- Build feature-engineering pipelines with ColumnTransformer  <br>- Deliver cross-validated Ames-Housing model|- Implement bidirectional LSTM for IMDB sentiment classification|
|**7**|28 Jul – 3 Aug|- Explore model interpretability (SHAP)  <br>- Generate SHAP plots for Week 6 housing model|- Enter transformers: fine-tune DistilBERT on SST-2 sentiment task|
|**8**|4 – 10 Aug|- Introduce time-series: Prophet & ARIMA  <br>- Forecast 30-day COVID-case trend|- Study autoencoders; train dense AE for credit-card anomaly detection|
|**9**|11 – 17 Aug|- Build recommender system via matrix factorisation on MovieLens-100K|- Learn GAN theory; train DCGAN on MNIST and generate samples|
|**10**|18 – 24 Aug|- Practise ensemble stacking & hyper-tuning; aim top-10 % in Kaggle Titanic|- Attempt CycleGAN or neural style-transfer on 100-image dataset|
|**11**|25 – 31 Aug|- Enter a Kaggle mini-competition; iterate ≥3 submissions, track leaderboard|- Package BERT fine-tune as FastAPI service and dockerise for deployment|
|**12**|1 – 7 Sep|- Capstone ML Project – Phase 1: define problem, ingest & clean data|- Capstone DL Project – Phase 1: dataset prep, establish baseline model|
|**13**|8 – 14 Sep|- Capstone ML – Phase 2: feature engineering & model search (AutoML/Tuning)|- Capstone DL – Phase 2: refine architecture, long-run training & logging|
|**14**|15 – 21 Sep|- Capstone ML – Phase 3: evaluation, interpretability, documentation|- Capstone DL – Phase 3: hyper-param sweep, optimise inference speed|
|**15**|22 – 28 Sep|- Conduct ML interview drills; polish GitHub, slides, blog posts|- Practise DL interview Qs (CNN, RNN, Transformer) & back-prop derivations; final portfolio review|

